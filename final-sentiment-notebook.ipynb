{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9052859,"sourceType":"datasetVersion","datasetId":5458493}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nimport os\nimport numpy as np\n# from tqdm import tqdm\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding, get_scheduler\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-07-28T16:26:29.657920Z","iopub.execute_input":"2024-07-28T16:26:29.658296Z","iopub.status.idle":"2024-07-28T16:26:36.279178Z","shell.execute_reply.started":"2024-07-28T16:26:29.658266Z","shell.execute_reply":"2024-07-28T16:26:36.278368Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stderr","text":"2024-07-28 16:26:33.537467: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-07-28 16:26:33.537523: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-07-28 16:26:33.538936: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Load Data","metadata":{}},{"cell_type":"code","source":"# def main(filepath, model_name, batchsize=32, lr=3e-5, num_epochs=3):\n# Load Data\nfilepath= \"/kaggle/input/transcription/final_transcription.csv\"\ndata = pd.read_csv(filepath)\ndata","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:26:36.280709Z","iopub.execute_input":"2024-07-28T16:26:36.281292Z","iopub.status.idle":"2024-07-28T16:26:36.304557Z","shell.execute_reply.started":"2024-07-28T16:26:36.281265Z","shell.execute_reply":"2024-07-28T16:26:36.303652Z"},"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"     Unnamed: 0                                      Transcription Sentiment\n0             0  Today's feature on home gardening channels exp...  Positive\n1             1  Our examination of the impact of gaming livest...  Negative\n2             2  This week's feature on pet rescue channels exp...  Positive\n3             3  Our report on the rise of unboxing videos exam...  Negative\n4             4  Today's feature on educational science channel...  Positive\n..          ...                                                ...       ...\n543         493  Our report on the rise of conspiracy theory vi...  Negative\n544         494  Today's feature on language learning channels ...  Positive\n545         495  Our examination of the impact of fitness influ...  Negative\n546         496  This week's feature on tiny house living explo...  Positive\n547         497  Our report on the rise of urban exploration vi...  Negative\n\n[548 rows x 3 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>Transcription</th>\n      <th>Sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>Today's feature on home gardening channels exp...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>Our examination of the impact of gaming livest...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>This week's feature on pet rescue channels exp...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>Our report on the rise of unboxing videos exam...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>Today's feature on educational science channel...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>543</th>\n      <td>493</td>\n      <td>Our report on the rise of conspiracy theory vi...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>544</th>\n      <td>494</td>\n      <td>Today's feature on language learning channels ...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>545</th>\n      <td>495</td>\n      <td>Our examination of the impact of fitness influ...</td>\n      <td>Negative</td>\n    </tr>\n    <tr>\n      <th>546</th>\n      <td>496</td>\n      <td>This week's feature on tiny house living explo...</td>\n      <td>Positive</td>\n    </tr>\n    <tr>\n      <th>547</th>\n      <td>497</td>\n      <td>Our report on the rise of urban exploration vi...</td>\n      <td>Negative</td>\n    </tr>\n  </tbody>\n</table>\n<p>548 rows Ã— 3 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Preprocess Data","metadata":{}},{"cell_type":"code","source":"data.drop(columns=[\"Unnamed: 0\"], inplace=True)\n\nlabel = {\"Negative\": 0, \"Positive\": 1}\ndata['Sentiment'] = data['Sentiment'].map(label)\n\n# Create Datasets\ndf_hf = Dataset.from_pandas(data)\ndf_hf = df_hf.train_test_split(test_size=0.1)\ndf_hf[\"validation\"] = df_hf.pop(\"test\")\ndf_hf","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:26:36.305603Z","iopub.execute_input":"2024-07-28T16:26:36.305900Z","iopub.status.idle":"2024-07-28T16:26:36.336108Z","shell.execute_reply.started":"2024-07-28T16:26:36.305874Z","shell.execute_reply":"2024-07-28T16:26:36.335268Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"DatasetDict({\n    train: Dataset({\n        features: ['Transcription', 'Sentiment'],\n        num_rows: 493\n    })\n    validation: Dataset({\n        features: ['Transcription', 'Sentiment'],\n        num_rows: 55\n    })\n})"},"metadata":{}}]},{"cell_type":"code","source":"def load_tokenizer_and_model(model_name, num_labels):\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    if tokenizer.pad_token is None:\n        if tokenizer.eos_token is not None:\n            tokenizer.pad_token = tokenizer.eos_token\n        else:\n            tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n        tokenizer.model_max_length = 512  # Adjust as needed\n    model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n    model.config.pad_token_id = tokenizer.pad_token_id  # Set pad_token_id in the model configuration\n    model.resize_token_embeddings(len(tokenizer))  # Resize the token embeddings to match the new tokenizer length\n    return tokenizer, model","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:26:36.337801Z","iopub.execute_input":"2024-07-28T16:26:36.338113Z","iopub.status.idle":"2024-07-28T16:26:36.344154Z","shell.execute_reply.started":"2024-07-28T16:26:36.338089Z","shell.execute_reply":"2024-07-28T16:26:36.343282Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"\ndef prepare_data_loaders(df_hf, tokenizer, batchsize):\n    def tokenize_function(batch):\n        return tokenizer(batch['Transcription'], truncation=True, padding=True)\n    \n    tokenized_df_hf = df_hf.map(tokenize_function, batched=True, batch_size=batchsize)\n    tokenized_df_hf = tokenized_df_hf.remove_columns(['Transcription'])\n    tokenized_df_hf = tokenized_df_hf.rename_column(\"Sentiment\", \"labels\")\n    tokenized_df_hf = tokenized_df_hf.with_format('torch')\n    \n    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n    \n    train_dataloader = DataLoader(\n        dataset=tokenized_df_hf['train'],\n        batch_size=batchsize,\n        shuffle=True,\n        collate_fn=collator\n    )\n    \n    val_dataloader = DataLoader(\n        dataset=tokenized_df_hf['validation'],\n        batch_size=batchsize,\n        collate_fn=collator\n    )\n    \n    return train_dataloader, val_dataloader","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:26:36.345295Z","iopub.execute_input":"2024-07-28T16:26:36.345628Z","iopub.status.idle":"2024-07-28T16:26:36.359504Z","shell.execute_reply.started":"2024-07-28T16:26:36.345597Z","shell.execute_reply":"2024-07-28T16:26:36.358738Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"def train_and_evaluate(model, train_dataloader, val_dataloader, num_epochs, lr):\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model.to(device)\n    \n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n    \n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    num_of_steps = num_epochs * len(train_dataloader)\n    lr_scheduler = get_scheduler(\n        \"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_of_steps\n    )\n    \n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for batch in train_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            train_loss += loss.item()\n        \n        avg_train_loss = train_loss / len(train_dataloader)\n        \n        model.eval()\n        val_loss = 0\n        preds = []\n        labels = []\n        with torch.no_grad():\n            for batch in val_dataloader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                outputs = model(**batch)\n                loss = outputs.loss\n                val_loss += loss.item()\n                preds.append(outputs.logits.cpu().numpy())\n                labels.append(batch['labels'].cpu().numpy())\n        \n        preds = np.concatenate(preds, axis=0)\n        labels = np.concatenate(labels, axis=0)\n        avg_val_loss = val_loss / len(val_dataloader)\n        accuracy, precision, recall, f1 = compute_metrics(preds, labels)\n        \n        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:26:36.360862Z","iopub.execute_input":"2024-07-28T16:26:36.361140Z","iopub.status.idle":"2024-07-28T16:26:36.373556Z","shell.execute_reply.started":"2024-07-28T16:26:36.361117Z","shell.execute_reply":"2024-07-28T16:26:36.372631Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def compute_metrics(preds, labels):\n    preds = np.argmax(preds, axis=1)\n    accuracy = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=1)\n    return accuracy, precision, recall, f1","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:26:36.374696Z","iopub.execute_input":"2024-07-28T16:26:36.375029Z","iopub.status.idle":"2024-07-28T16:26:36.387249Z","shell.execute_reply.started":"2024-07-28T16:26:36.374986Z","shell.execute_reply":"2024-07-28T16:26:36.386304Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Example usage\nmodel_name = 'bert-base-uncased'\nnum_labels = 2\nbatchsize = 16\nnum_epochs = 3\nlr = 2e-5\n\ntokenizer, model = load_tokenizer_and_model(model_name, num_labels)\ntrain_dataloader, val_dataloader = prepare_data_loaders(df_hf, tokenizer, batchsize)\ntrain_and_evaluate(model, train_dataloader, val_dataloader, num_epochs, lr)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Epoch 1/3, Train Loss: 0.5047, Val Loss: 0.2651\n\nAccuracy: 0.9273, Precision: 0.9286, Recall: 0.9273, F1 Score: 0.9266\n\nEpoch 2/3, Train Loss: 0.2437, Val Loss: 0.1777\n\nAccuracy: 0.9455, Precision: 0.9457, Recall: 0.9455, F1 Score: 0.9452\n\nEpoch 3/3, Train Loss: 0.1650, Val Loss: 0.1816\n\nAccuracy: 0.9455, Precision: 0.9457, Recall: 0.9455, F1 Score: 0.9452","metadata":{}},{"cell_type":"markdown","source":"## GPT2","metadata":{}},{"cell_type":"code","source":"# Example usage\nmodel_name = 'distilbert/distilgpt2'\nnum_labels = 2\nbatchsize = 16\nnum_epochs = 3\nlr = 2e-5\n\ntokenizer, model = load_tokenizer_and_model(model_name, num_labels)\ntrain_dataloader, val_dataloader = prepare_data_loaders(df_hf, tokenizer, batchsize)\ntrain_and_evaluate(model, train_dataloader, val_dataloader, num_epochs, lr)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Epoch 1/3, Train Loss: 0.6132, Val Loss: 0.3058\n\nAccuracy: 0.8909, Precision: 0.8909, Recall: 0.8909, F1 Score: 0.8909\n\nEpoch 2/3, Train Loss: 0.2611, Val Loss: 0.1862\n\nAccuracy: 0.9273, Precision: 0.9289, Recall: 0.9273, F1 Score: 0.9268\n\nEpoch 3/3, Train Loss: 0.2095, Val Loss: 0.1860\n\nAccuracy: 0.9273, Precision: 0.9289, Recall: 0.9273, F1 Score: 0.9268","metadata":{}},{"cell_type":"markdown","source":"## Bart ","metadata":{}},{"cell_type":"code","source":"# Example usage\nmodel_name = 'facebook/bart-base'\nnum_labels = 2\nbatchsize = 16\nnum_epochs = 3\nlr = 2e-5\n\ntokenizer, model = load_tokenizer_and_model(model_name, num_labels)\ntrain_dataloader, val_dataloader = prepare_data_loaders(df_hf, tokenizer, batchsize)\ntrain_and_evaluate(model, train_dataloader, val_dataloader, num_epochs, lr)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Epoch 1/3, Train Loss: 0.3634, Val Loss: 0.3105\n\nAccuracy: 0.9273, Precision: 0.9273, Recall: 0.9273, F1 Score: 0.9273\n\nEpoch 2/3, Train Loss: 0.2060, Val Loss: 0.2264\n\nAccuracy: 0.9273, Precision: 0.9273, Recall: 0.9273, F1 Score: 0.9273\n\nEpoch 3/3, Train Loss: 0.1469, Val Loss: 0.1995\n\nAccuracy: 0.9455, Precision: 0.9463, Recall: 0.9455, F1 Score: 0.9456","metadata":{}},{"cell_type":"markdown","source":"## Flan T5","metadata":{}},{"cell_type":"code","source":"# Example usage\nmodel_name = 'google/flan-t5-base'\nnum_labels = 2\nbatchsize = 8\nnum_epochs = 10\nlr = 2e-5\n\ntokenizer, model = load_tokenizer_and_model(model_name, num_labels)\ntrain_dataloader, val_dataloader = prepare_data_loaders(df_hf, tokenizer, batchsize)\ntrain_and_evaluate(model, train_dataloader, val_dataloader, num_epochs, lr)\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:47:00.087486Z","iopub.execute_input":"2024-07-28T16:47:00.088699Z","iopub.status.idle":"2024-07-28T16:52:27.287951Z","shell.execute_reply.started":"2024-07-28T16:47:00.088660Z","shell.execute_reply":"2024-07-28T16:52:27.286950Z"},"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at google/flan-t5-base and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/493 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7ff8f00af9004f798cd0cdae2b3b6e7a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/55 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5713a37c936043f89b6f4e6f401a0b6f"}},"metadata":{}},{"name":"stdout","text":"Epoch 1/10, Train Loss: 0.5665, Val Loss: 0.2344\nAccuracy: 0.9273, Precision: 0.9296, Recall: 0.9273, F1 Score: 0.9272\nEpoch 2/10, Train Loss: 0.2709, Val Loss: 0.1046\nAccuracy: 0.9455, Precision: 0.9461, Recall: 0.9455, F1 Score: 0.9455\nEpoch 3/10, Train Loss: 0.2287, Val Loss: 0.0752\nAccuracy: 0.9636, Precision: 0.9661, Recall: 0.9636, F1 Score: 0.9636\nEpoch 4/10, Train Loss: 0.1712, Val Loss: 0.0693\nAccuracy: 0.9818, Precision: 0.9825, Recall: 0.9818, F1 Score: 0.9818\nEpoch 5/10, Train Loss: 0.1521, Val Loss: 0.0876\nAccuracy: 0.9455, Precision: 0.9509, Recall: 0.9455, F1 Score: 0.9453\nEpoch 6/10, Train Loss: 0.1276, Val Loss: 0.0755\nAccuracy: 0.9818, Precision: 0.9824, Recall: 0.9818, F1 Score: 0.9818\nEpoch 7/10, Train Loss: 0.0976, Val Loss: 0.0711\nAccuracy: 0.9818, Precision: 0.9824, Recall: 0.9818, F1 Score: 0.9818\nEpoch 8/10, Train Loss: 0.1176, Val Loss: 0.0743\nAccuracy: 0.9636, Precision: 0.9636, Recall: 0.9636, F1 Score: 0.9636\nEpoch 9/10, Train Loss: 0.0920, Val Loss: 0.0734\nAccuracy: 0.9636, Precision: 0.9636, Recall: 0.9636, F1 Score: 0.9636\nEpoch 10/10, Train Loss: 0.0776, Val Loss: 0.0756\nAccuracy: 0.9636, Precision: 0.9636, Recall: 0.9636, F1 Score: 0.9636\n","output_type":"stream"}]},{"cell_type":"code","source":"def save_model(model, tokenizer, output_dir):\n    if not os.path.exists(output_dir):\n        os.makedirs(output_dir)\n    model.save_pretrained(output_dir)\n    tokenizer.save_pretrained(output_dir)\n    print(f\"Model and tokenizer saved to {output_dir}\")\n    \n# Save the model and tokenizer\nsave_model(model, tokenizer, \"model_ver2\")","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:52:27.290269Z","iopub.execute_input":"2024-07-28T16:52:27.290600Z","iopub.status.idle":"2024-07-28T16:52:28.770436Z","shell.execute_reply.started":"2024-07-28T16:52:27.290571Z","shell.execute_reply":"2024-07-28T16:52:28.769478Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Model and tokenizer saved to model_ver2\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load the saved model and tokenizer\nmodel_name = \"bert-base-uncased\"\nmodel_dir = \"/kaggle/working/\\.\"\ntokenizer = AutoTokenizer.from_pretrained(model_dir)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_dir)\n\n# Define the inference function\ndef predict_sentiment(text):\n    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True, max_length=512)\n    with torch.no_grad():\n        outputs = model(**inputs)\n    logits = outputs.logits\n    predicted_class_id = torch.argmax(logits, dim=1).item()\n    return \"Positive\" if predicted_class_id == 1 else \"Negative\"\n","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:42:28.912069Z","iopub.execute_input":"2024-07-28T16:42:28.912742Z","iopub.status.idle":"2024-07-28T16:42:29.680175Z","shell.execute_reply.started":"2024-07-28T16:42:28.912709Z","shell.execute_reply":"2024-07-28T16:42:29.679353Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"test_text = \"\"\"The new product launched by the company has been receiving rave reviews from customers. \nThey are particularly impressed with the innovative features and the user-friendly interface. \nMany users have reported that the product has significantly improved their productivity and efficiency. \nThe company's customer service has also been praised for their prompt and helpful responses to queries. \nOverall, the launch has been a huge success, with sales exceeding initial projections and positive feedback pouring in from all quarters. \nIt's clear that the company has managed to meet the needs and expectations of its customers with this latest offering.\"\"\"\n\npredict_sentiment(test_text)","metadata":{"execution":{"iopub.status.busy":"2024-07-28T16:44:38.387757Z","iopub.execute_input":"2024-07-28T16:44:38.388189Z","iopub.status.idle":"2024-07-28T16:44:38.737473Z","shell.execute_reply.started":"2024-07-28T16:44:38.388156Z","shell.execute_reply":"2024-07-28T16:44:38.736507Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"'Positive'"},"metadata":{}}]},{"cell_type":"code","source":"\n# Load Pre-trained Tokenizer and Model\ntokenizer = AutoTokenizer.from_pretrained(model_name)\nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n\n    # Tokenization Function\n    def tokenize_function(batch):\n        return tokenizer(batch['Transcription'], truncation=True, padding=True)\n\n    # Prepare Data Loaders\n    tokenized_df_hf = df_hf.map(tokenize_function, batched=True, batch_size=batchsize)\n    tokenized_df_hf = tokenized_df_hf.remove_columns(['Transcription'])\n    tokenized_df_hf = tokenized_df_hf.rename_column(\"Sentiment\", \"labels\")\n    tokenized_df_hf = tokenized_df_hf.with_format('torch')\n\n    collator = DataCollatorWithPadding(tokenizer=tokenizer)\n\n    train_dataloader = DataLoader(\n        dataset=tokenized_df_hf['train'],\n        batch_size=batchsize,\n        shuffle=True,\n        collate_fn=collator\n    )\n\n    val_dataloader = DataLoader(\n        dataset=tokenized_df_hf['validation'],\n        batch_size=batchsize,\n        collate_fn=collator\n    )\n\n    # Set Device\n    device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n    model.to(device)\n\n    # Set Parallelism Environment Variable\n    os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n\n    # Define Optimizer and Scheduler\n    optimizer = torch.optim.AdamW(model.parameters(), lr=lr)\n    num_of_steps = num_epochs * len(train_dataloader)\n    lr_scheduler = get_scheduler(\n        \"linear\",\n        optimizer=optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_of_steps\n    )\n\n# Train Model\n    for epoch in range(num_epochs):\n        model.train()\n        train_loss = 0\n        for batch in train_dataloader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            outputs = model(**batch)\n            loss = outputs.loss\n            loss.backward()\n            optimizer.step()\n            lr_scheduler.step()\n            optimizer.zero_grad()\n            train_loss += loss.item()\n            progress.update()\n        \n        avg_train_loss = train_loss / len(train_dataloader)\n        \n        # Evaluate Model\n        model.eval()\n        val_loss = 0\n        preds = []\n        labels = []\n        with torch.no_grad():\n            for batch in val_dataloader:\n                batch = {k: v.to(device) for k, v in batch.items()}\n                outputs = model(**batch)\n                loss = outputs.loss\n                val_loss += loss.item()\n                preds.append(outputs.logits.cpu().numpy())\n                labels.append(batch['labels'].cpu().numpy())\n        \n        preds = np.concatenate(preds, axis=0)\n        labels = np.concatenate(labels, axis=0)\n        avg_val_loss = val_loss / len(val_dataloader)\n        accuracy, precision, recall, f1 = compute_metrics(preds, labels)\n    \n        \n        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n        print(f\"Accuracy: {accuracy:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}, F1 Score: {f1:.4f}\")\n\ndef compute_metrics(preds, labels):\n    preds = np.argmax(preds, axis=1)\n    accuracy = accuracy_score(labels, preds)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted', zero_division=1)\n    return accuracy, precision, recall, f1\n","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}